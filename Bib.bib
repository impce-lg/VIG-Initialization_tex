
@article{liuRolePathVision2020,
  title = {Role, Path, and Vision of ``{{5G}} + {{BDS}}/{{GNSS}}''},
  author = {Liu, Jingnan and Gao, Kefu and Guo, Wenfei and Cui, Jingsong and Guo, Chi},
  year = {2020},
  month = dec,
  volume = {1},
  pages = {23},
  issn = {2662-1363},
  doi = {10.1186/s43020-020-00024-w},
  abstract = {Communication, positioning, navigation, and decision-making abilities have evolved into Positioning, Navigation, and Timing (PNT) intelligence during the long process of human migration and hence promoted human evolution. This article defines intelligence and smartness from the perspective of biological intelligence. New requirements as a result of the development of communication, navigation, time service, and decision making are identified in this study. The article points out that there are many radio PNT service methods, such as 5G, the new-generation highspeed communication networks and the low-latency and ubiquitous mobile communication networks as well as Global Navigation Satellite System (GNSS), but the integrated application is especially important in providing technical support for the adjustment and control of the physical world by intelligent sensing, cognition, decision-making, and precise coordination. The fusion of 5G and GNSS [including BeiDou Navigation Satellite System (BDS)] information with the corresponding equipment can be embedded into a machine to make it intelligent. Furthermore, the fused information of 5G and GNSS together with the environment information may extend human perception and physical world control ability in terms of time and space scale. It will help to develop critical information infrastructure in the age of intelligence, which will also extend the definition of artificial intelligence. Additionally, the ``5G\,+\,BDS/GNSS'' fusion path is analyzed explicitly herein in terms of realization methods, information processing, and new application services. On the whole, the application of ``5G\,+\,BDS/GNSS\,+\,satellite-based communication'' as a critical infrastructure for land, sea, air, space and network spatiotemporal control rights is proposed.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/YM9QWZPW/Liu 等。 - 2020 - Role, path, and vision of “5G + BDSGNSS”.pdf},
  journal = {Satellite Navigation},
  language = {en},
  number = {1}
}

@article{chiangPerformanceAnalysisINS2020a,
  title = {The {{Performance Analysis}} of {{INS}}/{{GNSS}}/{{V}}-{{SLAM Integration Scheme Using Smartphone Sensors}} for {{Land Vehicle Navigation Applications}} in {{GNSS}}-{{Challenging Environments}}},
  author = {Chiang, Kai-Wei and Le, Dinh Thuan and Duong, Thanh Trung and Sun, Rui},
  year = {2020},
  month = may,
  volume = {12},
  pages = {1732},
  issn = {2072-4292},
  doi = {10.3390/rs12111732},
  abstract = {Modern smartphones contain embedded global navigation satellite systems (GNSSs), inertial measurement units (IMUs), cameras, and other sensors which are capable of providing user position, velocity, and attitude. However, it is difficult to utilize the actual navigation performance capabilities of smartphones due to the low-cost and disparate sensors, software technologies adopted by manufacturers, and the significant influence of environmental conditions. In this study, we proposed a scheme that integrated sensor data from smartphone IMUs, GNSS chipsets, and cameras using an extended Kalman filter (EKF) to enhance the navigation performance. The visual data from the camera was preprocessed using oriented FAST (Features from accelerated segment test) and rotated BRIEF (Binary robust independent elementary features)-simultaneous localization and mapping (ORB-SLAM), rescaled by applying GNSS measurements, and converted to velocity data before being utilized to update the integration filter. In order to verify the performance of the integrated system, field test data was collected in a downtown area of Tainan City, Taiwan. Experimental results indicated that visual data contributed significantly to improving the accuracy of the navigation performance, demonstrating improvements of 43.0\% and 51.3\% in position and velocity, respectively. It was verified that the proposed integrated system, which used data from smartphone sensors, was efficient in terms of increasing navigation accuracy in GNSS-challenging environments.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/QVGZPZLH/Chiang 等。 - 2020 - The Performance Analysis of INSGNSSV-SLAM Integr.pdf},
  journal = {Remote Sensing},
  language = {en},
  number = {11}
}


@article{changGNSSINSLiDARSLAM2019a,
  title = {{{GNSS}}/{{INS}}/{{LiDAR}}-{{SLAM Integrated Navigation System Based}} on {{Graph Optimization}}},
  author = {Chang, Le and Niu, Xiaoji and Liu, Tianyi and Tang, Jian and Qian, Chuang},
  year = {2019},
  month = apr,
  volume = {11},
  pages = {1009},
  issn = {2072-4292},
  doi = {10.3390/rs11091009},
  abstract = {A Global Navigation Satellite System (GNSS)/Inertial Navigation System (INS)/Light Detection and Ranging (LiDAR)-Simultaneous Localization and Mapping (SLAM) integrated navigation system based on graph optimization is proposed and implemented in this paper. The navigation results are obtained by the information fusion of the GNSS position, Inertial Measurement Unit (IMU) preintegration result and the relative pose from the 3D probability map matching with graph optimizing. The sliding window method was adopted to ensure that the computational load of the graph optimization does not increase with time. Land vehicle tests were conducted, and the results show that the proposed GNSS/INS/LiDAR-SLAM integrated navigation system can effectively improve the navigation positioning accuracy compared to GNSS/INS and other current GNSS/INS/LiDAR methods. During the simulation of one-minute periods of GNSS outages, compared to the GNSS/INS integrated navigation system, the root mean square (RMS) of the position errors in the North and East directions of the proposed navigation system are reduced by approximately 82.2\% and 79.6\%, respectively, and the position error in the vertical direction and attitude errors are equivalent. Compared to the benchmark method of GNSS/INS/LiDAR-Google Cartographer, the RMS of the position errors in the North, East and vertical directions decrease by approximately 66.2\%, 63.1\% and 75.1\%, respectively, and the RMS of the roll, pitch and yaw errors are reduced by approximately 89.5\%, 92.9\% and 88.5\%, respectively. Furthermore, the relative position error during the GNSS outage periods is reduced to 0.26\% of the travel distance for the proposed method. Therefore, the GNSS/INS/LiDAR-SLAM integrated navigation system proposed in this paper can effectively fuse the information of GNSS, IMU and LiDAR and can significantly mitigate the navigation error, especially for cases of GNSS signal attenuation or interruption.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/5ZMJXS25/Chang 等。 - 2019 - GNSSINSLiDAR-SLAM Integrated Navigation System B.pdf},
  journal = {Remote Sensing},
  language = {en},
  number = {9}
}


@inproceedings{amraniAccurateInitializationMethod2019,
  title = {Accurate {{Initialization Method}} for {{Monocular Visual}}-{{Inertial SLAM}}},
  booktitle = {2019 3rd {{International Symposium}} on {{Autonomous Systems}} ({{ISAS}})},
  author = {Amrani, Abderraouf and Wang, Hesheng},
  year = {2019},
  month = may,
  pages = {159--164},
  publisher = {{IEEE}},
  address = {{Shanghai, China}},
  doi = {10.1109/ISASS.2019.8757790},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/5NB9WBTA/Amrani 和 Wang - 2019 - Accurate Initialization Method for Monocular Visua.pdf},
  isbn = {978-1-72811-297-8 978-1-72811-298-5}
}

@inproceedings{dominguez-contiVisualInertialSLAMInitialization2018,
  title = {Visual-{{Inertial SLAM Initialization}}: {{A General Linear Formulation}} and a {{Gravity}}-{{Observing Non}}-{{Linear Optimization}}},
  shorttitle = {Visual-{{Inertial SLAM Initialization}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{Mixed}} and {{Augmented Reality}} ({{ISMAR}})},
  author = {{Dominguez-Conti}, Javier and Yin, Jianfeng and Alami, Yacine and Civera, Javier},
  year = {2018},
  month = oct,
  pages = {37--45},
  publisher = {{IEEE}},
  address = {{Munich, Germany}},
  doi = {10.1109/ISMAR.2018.00027},
  abstract = {The initialization is one of the less reliable pieces of Visual-Inertial SLAM (VI-SLAM) and Odometry (VI-O). The estimation of the initial state (camera poses, IMU states and landmark positions) from the first data readings lacks the accuracy and robustness of other parts of the pipeline, and most algorithms have high failure rates and/or initialization delays up to tens of seconds. Such initialization is critical for AR systems, as the failures and delays of the current approaches can ruin the user experience or mandate impractical guided calibration.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/XUJZJC3T/Dominguez-Conti 等。 - 2018 - Visual-Inertial SLAM Initialization A General Lin.pdf},
  isbn = {978-1-5386-7459-8},
  language = {en}
}

@article{harsanyiMASATFastRobust2019,
  title = {{{MASAT}}: A Fast and Robust Algorithm for Pose-Graph Initialization},
  shorttitle = {{{MASAT}}},
  author = {Hars{\'a}nyi, K{\'a}roly and Kiss, Attila and Szir{\'a}nyi, Tam{\'a}s and Majdik, Andr{\'a}s},
  year = {2019},
  month = nov,
  pages = {S0167865519303241},
  issn = {01678655},
  doi = {10.1016/j.patrec.2019.11.010},
  abstract = {In this paper we propose a novel algorithm to compute the initial structure of pose-graph based Simultaneous Localization and Mapping (SLAM) systems. We perform a Breadth-First Search (BFS) on the graph in order to obtain multiple votes regarding the location of a certain robot position from all of its previously processed neighbors. Next, we define the initial location of a pose as the average of the multiple alternatives. By adopting the proposed initialization approach the number of iteration needed for optimization is significantly reduced while the computational complexity remains lightweight. The quantitative evaluation using six benchmark dataset shows the advantages of the proposed method.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/RVSERQLP/Harsányi 等。 - 2019 - MASAT a fast and robust algorithm for pose-graph .pdf},
  journal = {Pattern Recognition Letters},
  language = {en}
}

@article{hongVisualInertialOdometryRobust2018,
  title = {Visual-{{Inertial Odometry}} with {{Robust Initialization}} and {{Online Scale Estimation}}},
  author = {Hong, Euntae and Lim, Jongwoo},
  year = {2018},
  month = dec,
  volume = {18},
  pages = {4287},
  issn = {1424-8220},
  doi = {10.3390/s18124287},
  abstract = {Visual-inertial odometry (VIO) has recently received much attention for efficient and accurate ego-motion estimation of unmanned aerial vehicle systems (UAVs). Recent studies have shown that optimization-based algorithms achieve typically high accuracy when given enough amount of information, but occasionally suffer from divergence when solving highly non-linear problems. Further, their performance significantly depends on the accuracy of the initialization of inertial measurement unit (IMU) parameters. In this paper, we propose a novel VIO algorithm of estimating the motional state of UAVs with high accuracy. The main technical contributions are the fusion of visual information and pre-integrated inertial measurements in a joint optimization framework and the stable initialization of scale and gravity using relative pose constraints. To account for the ambiguity and uncertainty of VIO initialization, a local scale parameter is adopted in the online optimization. Quantitative comparisons with the state-of-the-art algorithms on the European Robotics Challenge (EuRoC) dataset verify the efficacy and accuracy of the proposed method.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/T53HFP5C/Hong 和 Lim - 2018 - Visual-Inertial Odometry with Robust Initializatio.pdf},
  journal = {Sensors},
  language = {en},
  number = {12}
}

@inproceedings{huangFastInitializationMethod2018,
  title = {A Fast Initialization Method of {{Visual}}-{{Inertial Odometry}} Based on Monocular Camera},
  booktitle = {2018 {{Ubiquitous Positioning}}, {{Indoor Navigation}} and {{Location}}-{{Based Services}} ({{UPINLBS}})},
  author = {Huang, Lixiao and Pan, Shuguo and Wang, Shuai and Zeng, Pan and Ye, Fei},
  year = {2018},
  month = mar,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Wuhan}},
  doi = {10.1109/UPINLBS.2018.8559929},
  abstract = {In recent years, due to the low cost and high precision characteristic of Visual-Inertial Odometry(VIO), VIO has become a hot research issue. However, the common visual and inertial joint positioning takes a lot of time for joint initialization, so improving the speed of the joint initialization is of great significance in real-time operate. This paper will propose a rapid method of joint initialization parameters. Compared with the traditional initialization method, the initialization method mentioned in this paper has a faster initialization speed with little effect on positioning accuracy. The scheme in the paper uses EuRoc dataset under monocular visual and IMU tight coupling system. The experimental results show that the initialization time can be controlled within 10 seconds using the method proposed in this paper.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/CQLTWIGK/Huang 等。 - 2018 - A fast initialization method of Visual-Inertial Od.pdf},
  isbn = {978-1-5386-3755-5},
  language = {en}
}

@article{huangOnlineInitializationExtrinsic2020,
  title = {Online {{Initialization}} and {{Extrinsic Spatial}}-{{Temporal Calibration}} for {{Monocular Visual}}-{{Inertial Odometry}}},
  author = {Huang, Weibo and Liu, Hong and Wan, Weiwei},
  year = {2020},
  month = apr,
  abstract = {This paper presents an online initialization method for bootstrapping the optimization-based monocular visualinertial odometry (VIO). The method can online calibrate the relative transformation (spatial) and time offsets (temporal) among camera and IMU, as well as estimate the initial values of metric scale, velocity, gravity, gyroscope bias, and accelerometer bias during the initialization stage. To compensate for the impact of time offset, our method includes two short-term motion interpolation algorithms for the camera and IMU pose estimation. Besides, it includes a three-step process to incrementally estimate the parameters from coarse to fine. First, the extrinsic rotation, gyroscope bias, and time offset are estimated by minimizing the rotation difference between the camera and IMU. Second, the metric scale, gravity, and extrinsic translation are approximately estimated by using the compensated camera poses and ignoring the accelerometer bias. Third, these values are refined by taking into account the accelerometer bias and the gravitational magnitude. For further optimizing the system states, a nonlinear optimization algorithm, which considers the time offset, is introduced for global and local optimization. Experimental results on public datasets show that the initial values and the extrinsic parameters, as well as the sensor poses, can be accurately estimated by the proposed method.},
  annote = {Comment: 15 pages, 7 figures},
  archivePrefix = {arXiv},
  eprint = {2004.05534},
  eprinttype = {arxiv},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/HT2RD6FR/Huang 等。 - 2020 - Online Initialization and Extrinsic Spatial-Tempor.pdf},
  journal = {arXiv:2004.05534 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{huangOnlineInitializationAutomatic2018,
  title = {Online {{Initialization}} and {{Automatic Camera}}-{{IMU Extrinsic Calibration}} for {{Monocular Visual}}-{{Inertial SLAM}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Huang, Weibo and Liu, Hong},
  year = {2018},
  month = may,
  pages = {5182--5189},
  publisher = {{IEEE}},
  address = {{Brisbane, QLD}},
  doi = {10.1109/ICRA.2018.8460206},
  abstract = {Most of the existing monocular visual-inertial SLAM techniques assume that the camera-IMU extrinsic parameters are known, therefore these methods merely estimate the initial values of velocity, visual scale, gravity, biases of gyroscope and accelerometer in the initialization stage. However, it's usually a professional work to carefully calibrate the extrinsic parameters, and it is required to repeat this work once the mechanical configuration of the sensor suite changes slightly. To tackle this problem, we propose an online initialization method to automatically estimate the initial values and the extrinsic parameters without knowing the mechanical configuration. The biases of gyroscope and accelerometer are considered in our method, and a convergence criteria for both orientation and translation calibration is introduced to identify the convergence and to terminate the initialization procedure. In the three processes of our method, an iterative strategy is firstly introduced to iteratively estimate the gyroscope bias and the extrinsic orientation. Secondly, the scale factor, gravity, and extrinsic translation are approximately estimated without considering the accelerometer bias. Finally, these values are further optimized by a refinement algorithm in which the accelerometer bias and the gravitational magnitude are taken into account. Extensive experimental results show that our method achieves competitive accuracy compared with the stateof-the-art with less calculation.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/EF93MVTZ/Huang 和 Liu - 2018 - Online Initialization and Automatic Camera-IMU Ext.pdf},
  isbn = {978-1-5386-3081-5},
  language = {en}
}

@inproceedings{liRapidRobustMonocular2019,
  title = {Rapid and {{Robust Monocular Visual}}-{{Inertial Initialization}} with {{Gravity Estimation}} via {{Vertical Edges}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Li, Jinyu and Bao, Hujun and Zhang, Guofeng},
  year = {2019},
  month = nov,
  pages = {6230--6236},
  publisher = {{IEEE}},
  address = {{Macau, China}},
  doi = {10.1109/IROS40897.2019.8968456},
  abstract = {Monocular visual-inertial tracking without good initialization easily fails due to its non-linear nature. Rapid and accurate metric initialization is crucial. In this paper, we propose a novel monocular visual-inertial initialization method which can initialize the IMU states, camera poses, and scale in a rapid and robust way. To avoid mixing gravity and accelerometer bias, we propose to use the detected vertical edges to estimate a better gravity. This improves the observability to the underlying problem even without sufficient movement, so we can solve all the states crucial for a good initialization. We evaluate our approach on EuRoC dataset and compare with existing state-of-the-art methods. The experimental results demonstrate the effectiveness of the proposed method.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/35CWQR22/Li 等。 - 2019 - Rapid and Robust Monocular Visual-Inertial Initial.pdf},
  isbn = {978-1-72814-004-9},
  language = {en}
}

@incollection{muScaleEstimationRefinement2017a,
  title = {Scale {{Estimation}} and {{Refinement}} in {{Monocular Visual}}-{{Inertial SLAM System}}},
  booktitle = {Image and {{Graphics}}},
  author = {Mu, Xufu and Chen, Jing and Leng, Zhen and Lin, Songnan and Huang, Ningsheng},
  editor = {Zhao, Yao and Kong, Xiangwei and Taubman, David},
  year = {2017},
  volume = {10666},
  pages = {533--544},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-71607-7_47},
  abstract = {The fusion of monocular visual and inertial cues has become popular in robotics, unmanned vehicle and augmented reality fields. Recent results have shown that optimization-based fusion strategies outperform filtering ones. The visual-inertial ORB-SLAM is optimization-based and has achieved great success. However, it takes all measurements into IMU initialization, which contains outliers, and it lacks of termination criterion. In this paper, we aim to resolve these issues. First, we present an approach to estimate scale, gravity and accelerometer bias together, and regard the estimated gravity as an indication for estimation convergence. Second, we propose a methodology that is able to use weight w derived from the robust norm for outliers handling, so that the estimated scale can be refined. We test our approaches with the public EuRoC datasets. Experimental results show that the proposed methods can achieve good scale estimation and refinement.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/P58MI45L/Mu 等。 - 2017 - Scale Estimation and Refinement in Monocular Visua.pdf},
  isbn = {978-3-319-71606-0 978-3-319-71607-7},
  language = {en}
}

@article{martinelliClosedFormSolutionVisualInertial2014,
  title = {Closed-{{Form Solution}} of {{Visual}}-{{Inertial Structure}} from {{Motion}}},
  author = {Martinelli, Agostino},
  year = {2014},
  month = jan,
  volume = {106},
  pages = {138--152},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-013-0647-7},
  abstract = {This paper investigates the visual-inertial structure from motion problem. A simple closed form solution to this problem is introduced. Special attention is devoted to identify the conditions under which the problem has a finite number of solutions. Specifically, it is shown that the problem can have a unique solution, two distinct solutions and infinite solutions depending on the trajectory, on the number of point-features and on their layout and on the number of camera images. The investigation is also performed in the case when the inertial data are biased, showing that, in this latter case, more images and more restrictive conditions on the trajectory are required for the problem resolvability.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/PC93PYI5/Martinelli - 2014 - Closed-Form Solution of Visual-Inertial Structure .pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {2}
}

@article{nutziFusionIMUVision2011,
  title = {Fusion of {{IMU}} and {{Vision}} for {{Absolute Scale Estimation}} in {{Monocular SLAM}}},
  author = {N{\"u}tzi, Gabriel and Weiss, Stephan and Scaramuzza, Davide and Siegwart, Roland},
  year = {2011},
  month = jan,
  volume = {61},
  pages = {287--299},
  issn = {0921-0296, 1573-0409},
  doi = {10.1007/s10846-010-9490-z},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/M8SKHJIN/Nützi 等。 - 2011 - Fusion of IMU and Vision for Absolute Scale Estima.pdf},
  journal = {Journal of Intelligent \& Robotic Systems},
  language = {en},
  number = {1-4}
}

@inproceedings{kneipDeterministicInitializationMetric2011,
  title = {Deterministic Initialization of Metric State Estimation Filters for Loosely-Coupled Monocular Vision-Inertial Systems},
  booktitle = {2011 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Kneip, L. and Weiss, S. and Siegwart, R.},
  year = {2011},
  month = sep,
  pages = {2235--2241},
  publisher = {{IEEE}},
  address = {{San Francisco, CA}},
  doi = {10.1109/IROS.2011.6094699},
  abstract = {In this work, we present a novel, deterministic closed-form solution for computing the scale factor and the gravity direction of a moving, loosely-coupled, and monocular vision-inertial system. The methodology is based on analysing delta-velocities. On one hand, they are obtained from a differentiation of the up-to-scale camera pose computation by a visual odometry or visual SLAM algorithm. On the other hand, they can also be retrieved from the gravity-affected short-term integration of acceleration signals. We derive a method for separating the gravity contribution and recovering the metric scale factor of the vision algorithm. The method thus also recovers the offset in roll and pitch angles of the vision reference frame with respect to the direction of the gravity vector. It uses only a single inertial integration period, and no absolute orientation information is required. For optimal sensor-fusion and metric scale-estimation filters in the loosely-coupled case, it has been shown that the convergence of the fusion of an upto-scale pose information with inertial measurements largely depends on the availability of a good initial value for the scale factor. We show how this problem can be tackled by applying the method presented in this paper. Finally, we present results in simulation and on real data, demonstrating the suitability of the method in real scenarios.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/QT57UUTA/Kneip 等。 - 2011 - Deterministic initialization of metric state estim.pdf},
  isbn = {978-1-61284-456-5 978-1-61284-454-1 978-1-61284-455-8},
  language = {en}
}


@inproceedings{qinRobustInitializationMonocular2017,
  title = {Robust Initialization of Monocular Visual-Inertial Estimation on Aerial Robots},
  booktitle = {2017 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Qin, Tong and Shen, Shaojie},
  year = {2017},
  month = sep,
  pages = {4225--4232},
  publisher = {{IEEE}},
  address = {{Vancouver, BC}},
  doi = {10.1109/IROS.2017.8206284},
  abstract = {In this paper, we propose a robust on-the-fly estimator initialization algorithm to provide high-quality initial states for monocular visual-inertial systems (VINS). Due to the non-linearity of VINS, a poor initialization can severely impact the performance of either filtering-based or graph-based methods. Our approach starts with a vision-only structure from motion (SfM) to build the up-to-scale structure of camera poses and feature positions. By loosely aligning this structure with pre-integrated IMU measurements, our approach recovers the metric scale, velocity, gravity vector, and gyroscope bias, which are treated as initial values to bootstrap the nonlinear tightly-coupled optimization framework. We highlight that our approach can perform on-the-fly initialization in various scenarios without using any prior information about system states and movement. The performance of the proposed approach is verified through the public UAV dataset and real-time onboard experiment. We make our implementation open source, which is the initialization part integrated in the VINS-Mono1.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/H3GMKR27/Qin 和 Shen - 2017 - Robust initialization of monocular visual-inertial.pdf},
  isbn = {978-1-5386-2682-5},
  language = {en}
}

@article{strasdatScaleDriftAwareLarge2010,
  title = {Scale {{Drift}}-{{Aware Large Scale Monocular SLAM}}},
  author = {Strasdat, Hauke and Montiel, J M M and Davison, Andrew J},
  year = {2010},
  pages = {8},
  abstract = {State of the art visual SLAM systems have recently been presented which are capable of accurate, large-scale and real-time performance, but most of these require stereo vision. Important application areas in robotics and beyond open up if similar performance can be demonstrated using monocular vision, since a single camera will always be cheaper, more compact and easier to calibrate than a multi-camera rig.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/LQW7EDUH/Strasdat 等。 - Scale Drift-Aware Large Scale Monocular SLAM.pdf},
  language = {en}
}

@article{xufumuAccurateInitialState2018,
  title = {Accurate {{Initial State Estimation}} in a {{Monocular Visual}}\textendash{{Inertial SLAM System}}},
  author = {{Xufu Mu} and {Jing Chen} and {Zixiang Zhou} and {Zhen Leng} and {Lei Fan}},
  year = {2018},
  month = feb,
  volume = {18},
  pages = {506},
  issn = {1424-8220},
  doi = {10.3390/s18020506},
  abstract = {The fusion of monocular visual and inertial cues has become popular in robotics, unmanned vehicles and augmented reality fields. Recent results have shown that optimization-based fusion strategies outperform filtering strategies. Robust state estimation is the core capability for optimization-based visual\textendash inertial Simultaneous Localization and Mapping (SLAM) systems. As a result of the nonlinearity of visual\textendash inertial systems, the performance heavily relies on the accuracy of initial values (visual scale, gravity, velocity and Inertial Measurement Unit (IMU) biases). Therefore, this paper aims to propose a more accurate initial state estimation method. On the basis of the known gravity magnitude, we propose an approach to refine the estimated gravity vector by optimizing the two-dimensional (2D) error state on its tangent space, then estimate the accelerometer bias separately, which is difficult to be distinguished under small rotation. Additionally, we propose an automatic termination criterion to determine when the initialization is successful. Once the initial state estimation converges, the initial estimated values are used to launch the nonlinear tightly coupled visual\textendash inertial SLAM system. We have tested our approaches with the public EuRoC dataset. Experimental results show that the proposed methods can achieve good initial state estimation, the gravity refinement approach is able to efficiently speed up the convergence process of the estimated gravity vector, and the termination criterion performs well.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/B7T3CA62/Xufu Mu 等。 - 2018 - Accurate Initial State Estimation in a Monocular V.pdf},
  journal = {Sensors},
  language = {en},
  number = {2}
}

@article{yangMonocularVisualInertial2017,
  title = {Monocular {{Visual}}\textendash{{Inertial State Estimation With Online Initialization}} and {{Camera}}\textendash{{IMU Extrinsic Calibration}}},
  author = {Yang, Zhenfei and Shen, Shaojie},
  year = {2017},
  month = jan,
  volume = {14},
  pages = {39--51},
  issn = {1545-5955, 1558-3783},
  doi = {10.1109/TASE.2016.2550621},
  abstract = {There have been increasing demands for developing microaerial vehicles with vision-based autonomy for search and rescue missions in complex environments. In particular, the monocular visual\textendash inertial system (VINS), which consists of only an inertial measurement unit (IMU) and a camera, forms a great lightweight sensor suite due to its low weight and small footprint. In this paper, we address two challenges for rapid deployment of monocular VINS: 1) the initialization problem and 2) the calibration problem. We propose a methodology that is able to initialize velocity, gravity, visual scale, and camera\textendash IMU extrinsic calibration on the fly. Our approach operates in natural environments and does not use any artificial markers. It also does not require any prior knowledge about the mechanical configuration of the system. It is a significant step toward plugand-play and highly customizable visual navigation for mobile robots. We show through online experiments that our method leads to accurate calibration of camera\textendash IMU transformation, with errors less than 0.02 m in translation and 1\textdegree{} in rotation. We compare out method with a state-of-the-art marker-based offline calibration method and show superior results. We also demonstrate the performance of the proposed approach in largescale indoor and outdoor experiments.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/R7QQFC2E/Yang 和 Shen - 2017 - Monocular Visual–Inertial State Estimation With On.pdf},
  journal = {IEEE Transactions on Automation Science and Engineering},
  language = {en},
  number = {1}
}

@inproceedings{faesslerAutomaticReinitializationFailure2015,
  title = {Automatic Re-Initialization and Failure Recovery for Aggressive Flight with a Monocular Vision-Based Quadrotor},
  booktitle = {2015 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Faessler, Matthias and Fontana, Flavio and Forster, Christian and Scaramuzza, Davide},
  year = {2015},
  month = may,
  pages = {1722--1729},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/ICRA.2015.7139420},
  abstract = {Autonomous, vision-based quadrotor flight is widely regarded as a challenging perception and control problem since the accuracy of a flight maneuver is strongly influenced by the quality of the on-board state estimate. In addition, any vision-based state estimator can fail due to the lack of visual information in the scene or due to the loss of feature tracking after an aggressive maneuver. When this happens, the robot should automatically re-initialize the state estimate to maintain its autonomy and, thus, guarantee the safety for itself and the environment. In this paper, we present a system that enables a monocular-vision\textendash based quadrotor to automatically recover from any unknown, initial attitude with significant velocity, such as after loss of visual tracking due to an aggressive maneuver. The recovery procedure consists of multiple stages, in which the quadrotor, first, stabilizes its attitude and altitude, then, re-initializes its visual state-estimation pipeline before stabilizing fully autonomously. To experimentally demonstrate the performance of our system, we aggressively throw the quadrotor in the air by hand and have it recover and stabilize all by itself. We chose this example as it simulates conditions similar to failure recovery during aggressive flight. Our system was able to recover successfully in several hundred throws in both indoor and outdoor environments.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/B7AIBRDM/Faessler 等。 - 2015 - Automatic re-initialization and failure recovery f.pdf},
  isbn = {978-1-4799-6923-4},
  language = {en}
}

@inproceedings{weissInertialOpticalFlow2015,
  title = {Inertial {{Optical Flow}} for {{Throw}}-and-{{Go Micro Air Vehicles}}},
  booktitle = {2015 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  author = {Weiss, Stephan and Brockers, Roland and Albrektsen, Sigurd and Matthies, Larry},
  year = {2015},
  month = jan,
  pages = {262--269},
  publisher = {{IEEE}},
  address = {{Waikoloa, HI, USA}},
  doi = {10.1109/WACV.2015.42},
  abstract = {In this paper, we describe a novel method using only optical flow from a single camera and inertial information to quickly initialize, deploy, and autonomously stabilize an inherently unstable aerial vehicle. Our approach requires a minimal number of tracked features in only two consecutive frames and inertial readings eliminating the need of long feature tracks or local maps and rendering it inherently failsafe. We show theoretically, in simulation, and in real experiments that we can reliably estimate and control the vehicle velocity, full attitude, and metric distance to the scene while self-calibrating inertial intrinsics and sensor extrinsics. In fact, the fast initialization, self-calibration, and inherent fail-safe property leads to the first visual-inertial throw-and-go capable system.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/YSQ36L8T/Weiss 等。 - 2015 - Inertial Optical Flow for Throw-and-Go Micro Air V.pdf},
  isbn = {978-1-4799-6683-7},
  language = {en}
}


@article{huangNewFastInMotion2018,
  title = {A {{New Fast In}}-{{Motion Coarse Alignment Method}} for {{GPS}}-{{Aided Low}}-{{Cost SINS}}},
  author = {Huang, Yulong and Zhang, Yonggang and Chang, Lubin},
  year = {2018},
  month = jun,
  volume = {23},
  pages = {1303--1313},
  issn = {1083-4435, 1941-014X},
  doi = {10.1109/TMECH.2018.2835486},
  abstract = {In this paper, a new fast in-motion coarse alignment method is proposed for a low-cost strap-down inertial navigation system (SINS) aided by the global positioning system (GPS). As compared with existing dynamic attitude estimation based initial alignment methods, in the proposed method, the constant attitude matrix from initial body frame to initial navigation frame is rapidly estimated, which results in improved alignment speed. Moreover, the time-varying attitude matrix from current body frame to initial body frame and gyroscope bias are jointly estimated based on a new constructed state-space model using the dynamic attitude estimation technique, and the resultant estimate and closed-loop calculation are utilized in the construction of vector observations, which improves the accuracy of coarse alignment. Experimental results illustrate that the proposed method has faster alignment speed and better alignment accuracy than existing state-of-the-art methods for GPS-aided low-cost SINS.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/WZAVSYPY/Huang 等。 - 2018 - A New Fast In-Motion Coarse Alignment Method for G.pdf},
  journal = {IEEE/ASME Transactions on Mechatronics},
  language = {en},
  number = {3}
}

@article{zhongAdaptiveInFlightAlignment2018,
  title = {Adaptive {{In}}-{{Flight Alignment}} of {{INS}}/{{GPS Systems}} for {{Aerial Mapping}}},
  author = {Zhong, Maiying and Guo, Jia and Zhou, Donghua},
  year = {2018},
  month = jun,
  volume = {54},
  pages = {1184--1196},
  issn = {0018-9251, 1557-9603, 2371-9877},
  doi = {10.1109/TAES.2017.2776058},
  abstract = {In this paper, a new in-flight alignment (IFA) method of the integrated inertial navigation system (INS) and global positioning system (GPS) is proposed for the aerial mapping applications. The integrated INS/GPS measurement system is used to provide attitude information and, based on this, the exterior orientation parameters can be derived for the purpose of direct georeference of the airborne imagery. The IFA plays an important role in achieving high accuracy of attitude estimation. However, the statistics of INS noise is usually time varying and degrades seriously the estimation accuracy in practice. In order to solve the problem, two strategies are taken into account in the paper. Firstly, an adaptive estimation algorithm is developed by adjusting the window size of data processing in IFA, so that the covariances of INS noise can be estimated and updated online to improve the state estimation performance. Secondly, a strong tracking filter is applied to guarantee the convergence of the IFA algorithm as well as its robustness against parameter perturbations and trajectory maneuvers. Finally, an aerial mapping experiment is implemented, and the results demonstrate the effectiveness of the proposed method.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/LR58HBAD/Zhong 等。 - 2018 - Adaptive In-Flight Alignment of INSGPS Systems fo.pdf},
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  language = {en},
  number = {3}
}

@article{changOptimizationbasedAlignmentStrapdown2016,
  title = {Optimization-Based Alignment for Strapdown Inertial Navigation System: Comparison and Extension},
  shorttitle = {Optimization-Based Alignment for Strapdown Inertial Navigation System},
  author = {Chang, Lubin and Li, Jingshu and Li, Kailong},
  year = {2016},
  month = aug,
  volume = {52},
  pages = {1697--1713},
  issn = {0018-9251},
  doi = {10.1109/TAES.2016.130824},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/S3P7MF9T/Chang 等。 - 2016 - Optimization-based alignment for strapdown inertia.pdf},
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  language = {en},
  number = {4}
}

@article{wuVelocityPositionIntegration2013b,
  title = {Velocity/{{Position Integration Formula Part I}}: {{Application}} to {{In}}-{{Flight Coarse Alignment}}},
  shorttitle = {Velocity/{{Position Integration Formula Part I}}},
  author = {Wu, Yuanxin and Pan, Xianfei},
  year = {2013},
  month = apr,
  volume = {49},
  pages = {1006--1023},
  issn = {0018-9251},
  doi = {10.1109/TAES.2013.6494395},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/X3G8IT3M/Wu 和 Pan - 2013 - VelocityPosition Integration Formula Part I Appl.pdf},
  journal = {IEEE Transactions on Aerospace and Electronic Systems},
  language = {en},
  number = {2}
}


@article{cioffiTightlycoupledFusionGlobal2020,
  title = {Tightly-Coupled {{Fusion}} of {{Global Positional Measurements}} in {{Optimization}}-Based {{Visual}}-{{Inertial Odometry}}},
  author = {Cioffi, Giovanni and Scaramuzza, Davide},
  year = {2020},
  month = jul,
  abstract = {Motivated by the goal of achieving robust, driftfree pose estimation in long-term autonomous navigation, in this work we propose a methodology to fuse global positional information with visual and inertial measurements in a tightly-coupled nonlinear-optimization\textendash based estimator. Differently from previous works, which are loosely-coupled, the use of a tightly-coupled approach allows exploiting the correlations amongst all the measurements. A sliding window of the most recent system states is estimated by minimizing a cost function that includes visual re-projection errors, relative inertial errors, and global positional residuals. We use IMU preintegration to formulate the inertial residuals and leverage the outcome of such algorithm to efficiently compute the global position residuals. The experimental results show that the proposed method achieves accurate and globally consistent estimates, with negligible increase of the optimization computational cost. Our method consistently outperforms the loosely-coupled fusion approach. The mean position error is reduced up to 50\% with respect to the loosely-coupled approach in outdoor Unmanned Aerial Vehicle (UAV) flights, where the global position information is given by noisy GPS measurements. To the best of our knowledge, this is the first work where global positional measurements are tightly fused in an optimization-based visualinertial odometry algorithm, leveraging the IMU preintegration method to define the global positional factors.},
  archivePrefix = {arXiv},
  eprint = {2003.04159},
  eprinttype = {arxiv},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/BK2VWWM4/Cioffi 和 Scaramuzza - 2020 - Tightly-coupled Fusion of Global Positional Measur.pdf},
  journal = {arXiv:2003.04159 [cs]},
  keywords = {Computer Science - Robotics},
  language = {en},
  primaryClass = {cs}
}


@inproceedings{tanskanenSemidirectEKFbasedMonocular2015,
  title = {Semi-Direct {{EKF}}-Based Monocular Visual-Inertial Odometry},
  booktitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Tanskanen, Petri and Naegeli, Tobias and Pollefeys, Marc and Hilliges, Otmar},
  year = {2015},
  month = sep,
  pages = {6073--6078},
  publisher = {{IEEE}},
  address = {{Hamburg, Germany}},
  doi = {10.1109/IROS.2015.7354242},
  abstract = {We propose a novel monocular visual inertial odometry algorithm that combines the advantages of EKFbased approaches with those of direct photometric error minimization methods. The method is based on sparse, very small patches and incorporates the minimization of photometric error directly into the EKF measurement model so that inertial data and vision-based surface measurements are used simultaneously during camera pose estimation. We fuse vision-based and inertial measurements almost at the raw-sensor level, allowing the estimated system state to constrain and guide imagespace measurements. Our formulation allows for an efficient implementation that runs in real-time on a standard CPU and has several appealing and unique characteristics such as being robust to fast camera motion, in particular rotation, and not depending on the presence of corner-like features in the scene. We experimentally demonstrate robust and accurate performance compared to ground truth and show that our method works on scenes containing only non-intersecting lines.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/BEYYPC77/Tanskanen 等。 - 2015 - Semi-direct EKF-based monocular visual-inertial od.pdf},
  isbn = {978-1-4799-9994-1},
  language = {en}
}

@article{Bloesch2017Iterated,
  title={Iterated extended Kalman filter based visual-inertial odometry using direct photometric feedback},
  author={Bloesch, Michael and Burri, Michael and Omari, Sammy and Hutter, Marco and Siegwart, Roland},
  journal={International Journal of Robotics Research},
  volume={36},
  number={10},
  pages={1053-1072},
  year={2017},
}

@article{liHighprecisionConsistentEKFbased2013,
  title = {High-Precision, Consistent {{EKF}}-Based Visual-Inertial Odometry},
  author = {Li, Mingyang and Mourikis, Anastasios I.},
  year = {2013},
  volume = {32},
  pages = {690--711},
  issn = {0278-3649},
  doi = {10.1177/0278364913481251},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/K85H2TVL/Li, Mourikis - 2013 - High-precision, consistent EKF-based visual-inertial odometry.pdf},
  journal = {The International Journal of Robotics Research},
  number = {6}
}

@article{leuteneggerKeyframeBasedVisualInertialSLAM2013,
  title = {Keyframe-{{Based Visual}}-{{Inertial SLAM Using Nonlinear Optimization}}},
  author = {Leutenegger, Stefan and Furgale, Paul and Rabaud, Vincent and Chli, Margarita and Konolige, Kurt and Siegwart, Roland},
  year = {2013},
  doi = {10.15607/RSS.2013.IX.037},
  abstract = {The fusion of visual and inertial cues has become popular in robotics due to the complementary nature of the two sensing modalities. While most fusion strategies to date rely on filtering schemes, the visual robotics community has recently turned to non-linear optimization approaches for tasks such as visual Simultaneous Localization And Mapping (SLAM), following the discovery that this comes with significant advantages in quality of performance and computational complexity. Following this trend, we present a novel approach to tightly integrate visual measurements with readings from an Inertial Measurement Unit (IMU) in SLAM. An IMU error term is integrated with the landmark reprojection error in a fully probabilistic manner, resulting to a joint non-linear cost function to be optimized. Employing the powerful concept of 'keyframes' we partially marginalize old states to maintain a bounded-sized optimization window, ensuring real-time operation. Comparing against both vision-only and loosely-coupled visual-inertial algorithms, our experiments confirm the benefits of tight fusion in terms of accuracy and robustness.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/4JQ2HM8A/Leutenegger et al. - 2013 - Keyframe-Based Visual-Inertial SLAM Using Nonlinear Optimization.pdf},
  journal = {Robotics: Science and Systems}
}

@article{mur-artalVisualInertialMonocularSLAM2017a,
  title = {Visual-{{Inertial Monocular SLAM With Map Reuse}}},
  author = {{Mur-Artal}, Raul and Tardos, Juan D.},
  year = {2017},
  month = apr,
  volume = {2},
  pages = {796--803},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2017.2653359},
  abstract = {In recent years there have been excellent results in visual-inertial odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However, these approaches lack the capability to close loops and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this letter, we present a novel tightly coupled visual-inertial simultaneous localization and mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of 1\% and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/L6HREYXY/Mur-Artal 和 Tardos - 2017 - Visual-Inertial Monocular SLAM With Map Reuse.pdf},
  journal = {IEEE Robotics and Automation Letters},
  language = {en},
  number = {2}
}

@article{qinVINSMonoRobustVersatile2017,
  title = {{{VINS}}-{{Mono}}: {{A Robust}} and {{Versatile Monocular Visual}}-{{Inertial State Estimator}}},
  author = {Qin, Tong and Li, Peiliang and Shen, Shaojie},
  year = {2017},
  pages = {1--17},
  issn = {15564967},
  doi = {10.1002/rob.21732},
  abstract = {A monocular visual-inertial system (VINS), consisting of a camera and a low-cost inertial measurement unit (IMU), forms the minimum sensor suite for metric six degrees-of-freedom (DOF) state estimation. However, the lack of direct distance measurement poses significant challenges in terms of IMU processing, estimator initialization, extrinsic calibration, and nonlinear optimization. In this work, we present VINS-Mono: a robust and versatile monocular visual-inertial state estimator.Our approach starts with a robust procedure for estimator initialization and failure recovery. A tightly-coupled, nonlinear optimization-based method is used to obtain high accuracy visual-inertial odometry by fusing pre-integrated IMU measurements and feature observations. A loop detection module, in combination with our tightly-coupled formulation, enables relocalization with minimum computation overhead.We additionally perform four degrees-of-freedom pose graph optimization to enforce global consistency. We validate the performance of our system on public datasets and real-world experiments and compare against other state-of-the-art algorithms. We also perform onboard closed-loop autonomous flight on the MAV platform and port the algorithm to an iOS-based demonstration. We highlight that the proposed work is a reliable, complete, and versatile system that is applicable for different applications that require high accuracy localization. We open source our implementations for both PCs and iOS mobile devices.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/LJC4P6BB/Qin, Li, Shen - 2017 - VINS-Mono A Robust and Versatile Monocular Visual-Inertial State Estimator.pdf}
}

@inproceedings{yuGPSaidedOmnidirectionalVisualInertial2019,
  title = {A {{GPS}}-Aided {{Omnidirectional Visual}}-{{Inertial State Estimator}} in {{Ubiquitous Environments}}},
  booktitle = {2019 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Yu, Yang and Gao, Wenliang and Liu, Chengju and Shen, Shaojie and Liu, Ming},
  year = {2019},
  month = nov,
  pages = {7750--7755},
  publisher = {{IEEE}},
  address = {{Macau, China}},
  doi = {10.1109/IROS40897.2019.8968519},
  abstract = {The visual-inertial navigation system (VINS) has been a practical approach for state estimation in recent years. In this paper, we propose a general GPS-aided omnidirectional visual-inertial state estimator capable of operating in ubiquitous environments and platforms. Our system consists of two parts: 1) the pre-processing of omnidirectional cameras, IMU, and GPS measurements, and 2) the sliding window based nonlinear optimization for accurate state estimation. We test our system in different conditions including an indoor office, campus roads, and challenging open water surface. Experiment results demonstrate the high accuracy of our approach than state-ofthe-art VINSs in all scenarios. The proposed odometry achieves drift ratio less than 0.5\% in 1200 m length outdoors campus road in overexposure conditions and 0.65\% in open water surface, without a loop closure, compared with a centimeter accuracy GPS reference.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/6G6TUI64/Yu 等。 - 2019 - A GPS-aided Omnidirectional Visual-Inertial State .pdf},
  isbn = {978-1-72814-004-9},
  language = {en}
}

@phdthesis{shinAccuracyImprovementLow,
  title = {Accuracy {{Improvement}} of {{Low Cost INS}}/{{GPS}} for {{Land Applications}}},
  author = {Shin, Eun-Hwan},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/VLT78WWY/Shin - Accuracy Improvement of Low Cost INSGPS for Land .pdf},
  language = {en}
}

@article{leeIntermittentGPSaidedVIO,
  title = {Intermittent {{GPS}}-Aided {{VIO}}: {{Online Initialization}} and {{Calibration}}},
  author = {Lee, Woosik and Eckenhoff, Kevin and Geneva, Patrick and Huang, Guoquan},
  pages = {9},
  abstract = {In this paper, we present an efficient and robust GPS-aided visual inertial odometry (GPS-VIO) system that fuses IMU-camera data with intermittent GPS measurements. To perform sensor fusion, spatiotemporal sensor calibration and initialization of the transform between the sensor reference frames are required. We propose an online calibration method for both the GPS-IMU extrinsics and time offset as well as a reference frame initialization procedure that is robust to GPS sensor noise. In addition, we prove the existence of four unobservable directions of the GPS-VIO system when estimating in the VIO reference frame, and advocate a state transformation to the GPS reference frame for full observability. We extensively evaluate the proposed approach in Monte-Carlo simulations where we investigate the system's robustness to different levels of GPS noise and loss of GPS signal, and additionally study the hyper-parameters used in the initialization procedure. Finally, the proposed system is validated in a large-scale real-world experiment.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/67AGSBFK/Lee 等。 - Intermittent GPS-aided VIO Online Initialization .pdf},
  language = {en}
}

@inproceedings{adeelResearchPerformanceAnalysis2017,
  title = {Research and Performance Analysis of Tightly Coupled Vision, {{INS}} and {{GNSS}} System for Land Vehicle Applications},
  booktitle = {30th {{International Technical Meeting}} of the {{Satellite Division}} of the {{Institute}} of {{Navigation}}, {{ION GNSS}} 2017},
  author = {Adeel, M. and Gong, Z. and Liu, P. and Wang, Y. and Chen, X.},
  year = {2017},
  volume = {5},
  pages = {3321--3330},
  abstract = {\textcopyright{} 2017 Institute of Navigation. All rights reserved. Global navigation satellite system (GNSS) is widely used for positioning and navigation. However, positioning accuracy of standalone GNSS system is badly affected in poor GNSS signal environments. Tightly coupled INS/GNSS has been studied and used to improve the positioning accuracy in poor GNSS signal environments. However, if GNSS signals are unavailable for long period of time, INS is unable to bound the sensor errors and positioning accuracy independently. Vision/IMU coupling is sued for indoor positioning in robots and provides positioning information in local coordinates. This paper is a research on the performance of tightly coupled vision, INS and GNSS system in environments when GNSS signals are good, poor or completely lost. Vision and IMU sensors are tightly coupled. IMU measurements are used to predict the INS navigation parameters and image features are used for measurement update. In tightly coupled Vision/INS system, navigation parameters are used to bound sensor errors. Predicted and updated INS navigation parameters are used in tightly coupled INS/GNSS system architecture to make further corrections in the INS predicted navigation parameters. In this paper we used Extended Kalman Filter (EKF) for integration. Positioning results of Vision/INS aided GNSS integration are compared against GNSS only, Vision/INS and GNSS/INS positioning results.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/6I9BX5WN/Adeel et al. - 2017 - Research and performance analysis of tightly coupled vision, INS and GNSS system for land vehicle applications.pdf}
}

@article{kimTightlycoupledIntegrationGPS2016,
  title = {Tightly-Coupled Integration of {{GPS}}/{{INS}} and Simultaneous Localisation and Mapping},
  author = {Kim, Jonghyuk and Cheng, Jiantong and Guivant, Jose},
  year = {2016},
  volume = {2016-Decem},
  pages = {194--199},
  issn = {14482053},
  abstract = {This paper presents a tightly integrated navi-gation system combining global positioning sys-tem (GPS) and inertial-based simultaneous lo-calization and mapping (SLAM) for UAV plat-forms. GPS raw measurements, called pseudor-ange and pseudorange rate, are directly fused to an inertial SLAM filter. A compressed form of unscented filtering is implemented by partition-ing the map into a local and global one. The performance of the proposed method is anal-ysed using a high-fidelity 6-degrees-of-freedom simulator, demonstrating accurate and robust navigation even under a single satellite obser-vation. The information gain of bearing and elevation angles is further analysed offering ef-fective sensing strategies.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/RRZS9TWB/Kim, Cheng, Guivant - 2016 - Tightly-coupled integration of GPSINS and simultaneous localisation and mapping.pdf},
  journal = {Australasian Conference on Robotics and Automation, ACRA}
}

@inproceedings{sahmoudiAnalysisNavigationSystem2016,
  title = {Analysis of a {{Navigation System}} Based on {{Partially Tight Integration}} of {{IMU}} -{{Visual Odometry}} with {{Loosely Coupled GPS}}},
  booktitle = {{{ION}}},
  author = {Sahmoudi, M and Ramuni, N},
  year = {2016},
  pages = {1322--1329},
  abstract = {\textemdash{} Reliable estimation of the pose is essential to make a UAV autonomous and practical in real life applications, such as surveying of infrastructure, safety and rescue, agriculture, etc. An accurate navigation capability is required that is robust and not solely dependent on GNSS. The goal of our proposed method is to implement a multi-sensor fusion architecture on the drone using low-cost GNSS receiver, a low-cost MEMS IMU and a mono camera. The paper presents a partial tight fusion architecture of IMU with the pose from monocular Visual odometry (VO) using an extended Kalman filter (EKF). We impose the state transition from the IMU and coupled with linear measurements from the VO. We further propose a loosely coupled architecture for the fusion of estimated pose from partial tight fusion, with the GPS using a linear Kalman filter. Compared with the traditional way of fusing Inertial-Vision or Inertial \textendash GPS, the proposed method is a balance of both the methods as it involves two step Kalman filter and capable of reliable estimation of pose in GNSS harsh environment (deep urban and indoor) and Vision outages and thereby reducing the drift of the IMU. The adopted method of VO does not need to estimate explicitly the features 3D coordinates. Alternatively, we exploit the epipolar geometry and trifocal tensor between each three successive camera views as geometric constraints to be injected in the measurements model of a linear Kalman filter. This formulation is implemented over a sliding window to avoid a full SLAM with high complexity.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/7JM7FKYP/Sahmoudi, Ramuni - 2016 - Analysis of a Navigation System based on Partially Tight Integration of IMU -Visual Odometry with Loosely Coup.pdf},
  keywords = {EKF,Gps aided navigation,Index Terms— Visual odometry,loosely coupled,tightly coupled,vision aided inertial navigation}
}

@article{shunsukeGNSSINSOnboard2015,
  title = {{{GNSS}}/{{INS}}/{{On}}-Board {{Camera Integration}} for {{Vehicle Self}}-{{Localization}} in {{Urban Canyon}}},
  author = {Shunsuke, Kamijo and Yanlei, Gu and Hsu, Li Ta},
  year = {2015},
  volume = {2015-Octob},
  pages = {2533--2538},
  issn = {2153-0009},
  doi = {10.1109/ITSC.2015.407},
  abstract = {\textemdash This paper presents a precise vehicle self-localization system for autonomous driving. The developed system integrates multiple on-board passive sensors, Global Navigation Satellite System (GNSS), Inertial Navigation System (INS) and on-board monocular camera, in order to achieve lane-level localization performance in urban environment. GNSS based positioning technique suffers from the effects of multipath and Non-Line-Of-Sight (NLOS) propagation in urban canyon. The positioning error also affects the performance in the integrated GNSS/INS system. In the other side, the lane-marking on road surface provides the important visual source of information for driving. This paper proposes to detect the occupied lane of vehicle using on-board cameras. The lane detection result is integrated with GNSS/INS system in order to improve the positioning error. The experiment results demonstrate that the proposed method can provide 90\% correctness with respect to the occupied lane.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/7BDBP842/Shunsuke, Yanlei, Hsu - 2015 - GNSSINSOn-board Camera Integration for Vehicle Self-Localization in Urban Canyon(2).pdf},
  journal = {IEEE Conference on Intelligent Transportation Systems, Proceedings, ITSC},
  keywords = {Integration,Lane Detection,Vehicle Self-Localization,Vision}
}

@article{chenIntegrationLowcostGNSS2018,
  title = {Integration of Low-Cost {{GNSS}} and Monocular Cameras for Simultaneous Localization and Mapping},
  author = {Chen, Xiao and Hu, Weidong and Zhang, Lefeng and Shi, Zhiguang and Li, Maisi},
  year = {2018},
  volume = {18},
  pages = {1--18},
  issn = {14248220},
  doi = {10.3390/s18072193},
  abstract = {Low-cost Global Navigation Satellite System (GNSS) receivers and monocular cameras are widely used in daily activities. The complementary nature of these two devices is ideal for outdoor navigation. In this paper, we investigate the integration of GNSS and monocular camera measurements in a simultaneous localization and mapping system. The proposed system first aligns the coordinates between two sensors. Subsequently, the measurements are fused by an optimization-based scheme. Our system can function in real-time and obtain the absolute position, scale, and attitude of the vehicle. It achieves a high accuracy without a preset map and also has the capability to work with a preset map. The system can easily be extended to create other forms of maps or for other types of cameras. Experimental results on a popular public dataset are presented to validate the performance of the proposed system.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/J6DEY98A/Chen et al. - 2018 - Integration of low-cost GNSS and monocular cameras for simultaneous localization and mapping.pdf},
  journal = {Sensors (Switzerland)},
  keywords = {Bundle adjustment,GNSS,Low-cost,Navigation,ORB-SLAM,Sensor fusion,SLAM},
  number = {7}
}

@article{wangVIMOVisualInertialMagneticNavigation2020,
  title = {{{VIMO}}: {{A Visual}}-{{Inertial}}-{{Magnetic Navigation System Based}} on {{Non}}-{{Linear Optimization}}},
  shorttitle = {{{VIMO}}},
  author = {Wang, Jingzhe and Li, Leilei and Yu, Huan and Gui, Xunya and Li, Zucheng},
  year = {2020},
  month = aug,
  volume = {20},
  pages = {4386},
  issn = {1424-8220},
  doi = {10.3390/s20164386},
  abstract = {Visual-inertial navigation systems are credited with superiority over both pure visual approaches and filtering ones. In spite of the high precision many state-of-the-art schemes have attained, yaw remains unobservable in those systems all the same. More accurate yaw estimation not only means more accurate attitude calculation but also leads to better position estimation. This paper presents a novel scheme that combines visual and inertial measurements as well as magnetic information for suppressing deviation in yaw. A novel method for initializing visual-inertial-magnetic odometers, which recovers the directions of magnetic north and gravity, the visual scalar factor, inertial measurement unit (IMU) biases etc., has been conceived, implemented, and validated. Based on non-linear optimization, a magnetometer cost function is incorporated into the overall optimization objective function as a yawing constraint among others. We have done extensive research and collected several datasets recorded in large-scale outdoor environments to certify the proposed system's viability, robustness, and performance. Cogent experiments and quantitative comparisons corroborate the merits of the proposed scheme and the desired effect of the involvement of magnetic information on the overall performance.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/NIDP8PZW/Wang 等。 - 2020 - VIMO A Visual-Inertial-Magnetic Navigation System.pdf},
  journal = {Sensors},
  language = {en},
  number = {16}
}


@inproceedings{mascaroGOMSFGraphOptimizationBased2018,
  title = {{{GOMSF}}: {{Graph}}-{{Optimization Based Multi}}-{{Sensor Fusion}} for Robust {{UAV Pose}} Estimation},
  shorttitle = {{{GOMSF}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Mascaro, Ruben and Teixeira, Lucas and Hinzmann, Timo and Siegwart, Roland and Chli, Margarita},
  year = {2018},
  month = may,
  pages = {1421--1428},
  publisher = {{IEEE}},
  address = {{Brisbane, QLD}},
  doi = {10.1109/ICRA.2018.8460193},
  abstract = {Achieving accurate, high-rate pose estimates from proprioceptive and/or exteroceptive measurements is the first step in the development of navigation algorithms for agile mobile robots such as Unmanned Aerial Vehicles (UAVs). In this paper, we propose a decoupled Graph-Optimization based Multi-Sensor Fusion approach (GOMSF) that combines generic 6 Degree-of-Freedom (DoF) visual-inertial odometry poses and 3 DoF globally referenced positions to infer the global 6 DoF pose of the robot in real-time. Our approach casts the fusion as a real-time alignment problem between the local base frame of the visual-inertial odometry and the global base frame. The alignment transformation that relates these coordinate systems is continuously updated by optimizing a sliding window pose graph containing the most recent robot's states. We evaluate the presented pose estimation method on both simulated data and large outdoor experiments using a small UAV that is capable to run our system onboard. Results are compared against different state-of-the-art sensor fusion frameworks, revealing that the proposed approach is substantially more accurate than other decoupled fusion strategies. We also demonstrate comparable results in relation with a finely tuned Extended Kalman Filter that fuses visual, inertial and GPS measurements in a coupled way and show that our approach is generic enough to deal with different input sources in a straightforward manner.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/Q9BHRGSZ/Mascaro 等。 - 2018 - GOMSF Graph-Optimization Based Multi-Sensor Fusio.pdf},
  isbn = {978-1-5386-3081-5},
  language = {en}
}

@book{farrellAidedNavigationGPS2008,
  title = {Aided Navigation: {{GPS}} with High Rate Sensors},
  shorttitle = {Aided Navigation},
  author = {Farrell, Jay},
  year = {2008},
  publisher = {{McGraw-Hill}},
  address = {{New York}},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/IJAZL5L4/Farrell - 2008 - Aided navigation GPS with high rate sensors.pdf},
  isbn = {978-0-07-149329-1},
  keywords = {Global Positioning System,Remote sensing},
  language = {en},
  lccn = {G109.5 .F367 2008},
  note = {OCLC: ocn212908814},
  series = {Electronic Engineering}
}

@article{forsterOnManifoldPreintegrationRealTime2017,
  title = {On-{{Manifold Preintegration}} for {{Real}}-{{Time Visual}}-{{Inertial Odometry}}},
  author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
  year = {2017},
  volume = {33},
  pages = {1--21},
  issn = {15523098},
  doi = {10.1109/TRO.2016.2597321},
  abstract = {Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time, this problem is further emphasized by the fact that inertial measurements come at high rate, hence leading to fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a \$\textbackslash backslash\$emph\{preintegration theory\} that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the \$\textbackslash backslash\$emph\{maximum a posteriori\} state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a-posteriori bias correction in analytic form. The second contribution is to show that the preintegrated IMU model can be seamlessly integrated into a visual-inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a \$\textbackslash backslash\$emph\{structureless\} model for visual measurements, which avoids optimizing over the 3D points, further accelerating the computation. We perform an extensive evaluation of our monocular \$\textbackslash backslash\$VIO pipeline on real and simulated datasets. The results confirm that our modelling effort leads to accurate state estimation in real-time, outperforming state-of-the-art approaches.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/J2AIAEYG/Forster et al. - 2017 - On-Manifold Preintegration for Real-Time Visual-Inertial Odometry.pdf},
  journal = {IEEE Transactions on Robotics},
  keywords = {Computer vision,sensor fusion,visual-inertial odometry (VIO)},
  number = {1},
  pmid = {15523098}
}


@inproceedings{Godha2006PerformanceEO,
  title={Performance evaluation of low cost MEMS-based IMU integrated with GPS for land vehicle navigation application},
  author={S. Godha},
  year={2006}
}

@article{loeligerIntroductionFactorGraphs2008,
  title = {An {{Introduction}} to {{Factor Graphs}}},
  author = {Loeliger, Hans-Andrea},
  year = {2008},
  pages = {245--246},
  issn = {10535888},
  doi = {10.1109/MSP.2004.1267047},
  abstract = {Presentation at the Second International Workshop on Machine Learning in Systems Biology},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/S4I4NZFN/Loeliger - 2008 - An Introduction to Factor Graphs.pdf},
  journal = {Teacher},
  pmid = {22409607}
}

@incollection{kaessBayesTreeAlgorithmic2010,
  title = {The {{Bayes Tree}}: {{An Algorithmic Foundation}} for {{Probabilistic Robot Mapping}}},
  shorttitle = {The {{Bayes Tree}}},
  booktitle = {Algorithmic {{Foundations}} of {{Robotics IX}}},
  author = {Kaess, Michael and Ila, Viorela and Roberts, Richard and Dellaert, Frank},
  editor = {Siciliano, Bruno and Khatib, Oussama and Groen, Frans and Hsu, David and Isler, Volkan and Latombe, Jean-Claude and Lin, Ming C.},
  year = {2010},
  volume = {68},
  pages = {157--173},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-17452-0_10},
  abstract = {We present a novel data structure, the Bayes tree, that provides an algorithmic foundation enabling a better understanding of existing graphical model inference algorithms and their connection to sparse matrix factorization methods. Similar to a clique tree, a Bayes tree encodes a factored probability density, but unlike the clique tree it is directed and maps more naturally to the square root information matrix of the simultaneous localization and mapping (SLAM) problem. In this paper, we highlight three insights provided by our new data structure. First, the Bayes tree provides a better understanding of batch matrix factorization in terms of probability densities. Second, we show how the fairly abstract updates to a matrix factorization translate to a simple editing of the Bayes tree and its conditional densities. Third, we apply the Bayes tree to obtain a completely novel algorithm for sparse nonlinear incremental optimization, that combines incremental updates with fluid relinearization of a reduced set of variables for efficiency, combined with fast convergence to the exact solution. We also present a novel strategy for incremental variable reordering to retain sparsity. We evaluate our algorithm on standard datasets in both landmark and pose SLAM settings.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/5JQNBVF8/Kaess 等。 - 2010 - The Bayes Tree An Algorithmic Foundation for Prob.pdf},
  isbn = {978-3-642-17451-3 978-3-642-17452-0},
  language = {en}
}

@article{kaessISAM2IncrementalSmoothing2012,
  title = {{{iSAM2}}: {{Incremental}} Smoothing and Mapping Using the {{Bayes}} Tree},
  shorttitle = {{{iSAM2}}},
  author = {Kaess, Michael and Johannsson, Hordur and Roberts, Richard and Ila, Viorela and Leonard, John J and Dellaert, Frank},
  year = {2012},
  month = feb,
  volume = {31},
  pages = {216--235},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364911430419},
  abstract = {We present a novel data structure, the Bayes tree, that provides an algorithmic foundation enabling a better understanding of existing graphical model inference algorithms and their connection to sparse matrix factorization methods. Similar to a clique tree, a Bayes tree encodes a factored probability density, but unlike the clique tree it is directed and maps more naturally to the square root information matrix of the simultaneous localization and mapping (SLAM) problem. In this paper, we highlight three insights provided by our new data structure. First, the Bayes tree provides a better understanding of the matrix factorization in terms of probability densities. Second, we show how the fairly abstract updates to a matrix factorization translate to a simple editing of the Bayes tree and its conditional densities. Third, we apply the Bayes tree to obtain a completely novel algorithm for sparse nonlinear incremental optimization, named iSAM2, which achieves improvements in efficiency through incremental variable re-ordering and fluid relinearization, eliminating the need for periodic batch steps. We analyze various properties of iSAM2 in detail, and show on a range of real and simulated datasets that our algorithm compares favorably with other recent mapping algorithms in both quality and efficiency.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/P74KVZUF/Kaess 等。 - 2012 - iSAM2 Incremental smoothing and mapping using the.pdf},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {2}
}

@article{indelmanInformationFusionNavigation2013,
  title = {Information Fusion in Navigation Systems via Factor Graph Based Incremental Smoothing},
  author = {Indelman, Vadim and Williams, Stephen and Kaess, Michael and Dellaert, Frank},
  year = {2013},
  month = aug,
  volume = {61},
  pages = {721--738},
  issn = {09218890},
  doi = {10.1016/j.robot.2013.05.001},
  abstract = {This paper presents a new approach for high-rate information fusion in modern inertial navigation systems, that have a variety of sensors operating at different frequencies. Optimal information fusion corresponds to calculating the maximum a posteriori estimate over the joint probability distribution function (pdf) of all states, a computationally-expensive process in the general case. Our approach consists of two key components, which yields a flexible, high-rate, near-optimal inertial navigation system. First, the joint pdf is represented using a graphical model, the factor graph, that fully exploits the system sparsity and provides a plug and play capability that easily accommodates the addition and removal of measurement sources. Second, an efficient incremental inference algorithm over the factor graph is applied, whose performance approaches the solution that would be obtained by a computationally-expensive batch optimization at a fraction of the computational cost. To further aid highrate performance, we introduce an equivalent IMU factor based on a recently developed technique for IMU pre-integration, drastically reducing the number of states that must be added to the system. The proposed approach is experimentally validated using real IMU and imagery data that was recorded by a ground vehicle, and a statistical performance study is conducted in a simulated aerial scenario. A comparison to conventional fixed-lag smoothing demonstrates that our method provides a considerably improved trade-off between computational complexity and performance.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/YUHFGGN9/Indelman 等。 - 2013 - Information fusion in navigation systems via facto.pdf},
  journal = {Robotics and Autonomous Systems},
  language = {en},
  number = {8}
}

@article{dellaertFactorGraphsRobot2017,
  title = {Factor {{Graphs}} for {{Robot Perception}}},
  author = {Dellaert, Frank and Kaess, Michael},
  year = {2017},
  volume = {6},
  pages = {1--139},
  issn = {1935-8253, 1935-8261},
  doi = {10.1561/2300000043},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/MBUXM7TW/Dellaert 和 Kaess - 2017 - Factor Graphs for Robot Perception.pdf},
  journal = {Foundations and Trends in Robotics},
  language = {en},
  number = {1-2}
}

@article{hornClosedformSolutionAbsolute1987,
  title = {Closed-Form Solution of Absolute Orientation Using Unit Quaternions},
  author = {Horn, Berthold K. P.},
  year = {1987},
  volume = {4},
  pages = {629},
  issn = {1084-7529},
  doi = {10.1364/JOSAA.4.000629},
  abstract = {Finding the relationship between two coordinate systems using pairs of measurements of the coordinates of a number of points in both systems is a classic photogrammetric task. It finds applications in stereophotogrammetry and in robotics. I present here a closed-form solution to the least-squares problem for three or more points. Currently various empirical, graphical, and numerical iterative methods are in use. Derivation of the solution is simplified by use of unit quaternions to represent rotation. I emphasize a symmetry property that a solution to this problem ought to possess. The best translational offset is the difference between the centroid of the coordinates in one system and the rotated and scaled centroid of the coordinates in the other system. The best scale is equal to the ratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids. These exact results are to be preferred to approximate methods based on measurements of a few selected points. The unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue of a symmetric 4 \texttimes{} 4 matrix. The elements of this matrix are combinations of sums of products of corresponding coordinates of the points.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/3US293Q2/Horn - 1987 - Closed-form solution of absolute orientation using unit quaternions.pdf},
  journal = {Journal of the Optical Society of America A},
  number = {4}
}

@inproceedings{rehderExtendingKalibrCalibrating2016,
  title = {Extending Kalibr: {{Calibrating}} the Extrinsics of Multiple {{IMUs}} and of Individual Axes},
  shorttitle = {Extending Kalibr},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Rehder, Joern and Nikolic, Janosch and Schneider, Thomas and Hinzmann, Timo and Siegwart, Roland},
  year = {2016},
  month = may,
  pages = {4304--4311},
  publisher = {{IEEE}},
  address = {{Stockholm, Sweden}},
  doi = {10.1109/ICRA.2016.7487628},
  abstract = {An increasing number of robotic systems feature multiple inertial measurement units (IMUs). Due to competing objectives\textemdash either desired vicinity to the center of gravity when used in controls, or an unobstructed field of view when integrated in a sensor setup with an exteroceptive sensor for ego-motion estimation\textemdash individual IMUs are often mounted at considerable distance. As a result, they sense different accelerations when the platform is subjected to rotational motions. In this work, we derive a method for spatially calibrating multiple IMUs in a single estimator based on the open-source camera/IMU calibration toolbox kalibr. We further extend the toolbox to determine IMU intrinsics, enabling accurate calibration of low-cost IMUs. The results suggest that the extended estimator is capable of precisely determining these intrinsics and even of localizing individual accelerometer axes inside a commercial grade IMU to millimeter precision.},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/QM9XQZK6/Rehder 等。 - 2016 - Extending kalibr Calibrating the extrinsics of mu.pdf},
  isbn = {978-1-4673-8026-3},
  language = {en}
}

@article{burriEuRoCMicroAerial2016a,
  title = {The {{EuRoC}} Micro Aerial Vehicle Datasets},
  author = {Burri, Michael and Nikolic, Janosch and Gohl, Pascal and Schneider, Thomas and Rehder, Joern and Omari, Sammy and Achtelik, Markus W and Siegwart, Roland},
  year = {2016},
  month = sep,
  volume = {35},
  pages = {1157--1163},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364915620033},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/U6H4AA5X/Burri 等。 - 2016 - The EuRoC micro aerial vehicle datasets.pdf},
  journal = {The International Journal of Robotics Research},
  language = {en},
  number = {10}
}

@article{camposInertialOnlyOptimizationVisualInertial2020,
  title = {Inertial-{{Only Optimization}} for {{Visual}}-{{Inertial Initialization}}},
  author = {Campos, Carlos and Montiel, Jos{\'e} M. M. and Tard{\'o}s, Juan D.},
  year = {2020},
  month = mar,
  abstract = {We formulate for the first time visual-inertial initialization as an optimal estimation problem, in the sense of maximum-a-posteriori (MAP) estimation. This allows us to properly take into account IMU measurement uncertainty, which was neglected in previous methods that either solved sets of algebraic equations, or minimized ad-hoc cost functions using least squares. Our exhaustive initialization tests on EuRoC dataset show that our proposal largely outperforms the best methods in the literature, being able to initialize in less than 4 seconds in almost any point of the trajectory, with a scale error of 5.3\% on average. This initialization has been integrated into ORB-SLAM Visual-Inertial boosting its robustness and efficiency while maintaining its excellent accuracy.},
  archiveprefix = {arXiv},
  eprint = {2003.05766},
  eprinttype = {arxiv},
  file = {/home/huanhexiao/MyStorage/Zotero/storage/J4YQCU3J/Campos 等。 - 2020 - Inertial-Only Optimization for Visual-Inertial Ini.pdf},
  journal = {arXiv:2003.05766 [cs]},
  keywords = {Computer Science - Robotics},
  language = {en},
  note = {Comment: 2020 International Conference on Robotics and Automation},
  primaryclass = {cs}
}
